{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7e543bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlglot\n",
    "from sqlglot import exp\n",
    "from sql_nameguard.analyze import SQLAnalyzer\n",
    "\n",
    "class SSCSCalculator:\n",
    "    def __init__(self):\n",
    "        # Configuration for weights\n",
    "        self.weights = {\n",
    "            exp.Join: 1,\n",
    "            exp.Where: 1,\n",
    "            exp.Group: 1,\n",
    "            exp.Having: 1,\n",
    "            exp.Order: 1,\n",
    "            exp.Case: 2,           # Branching logic = higher load\n",
    "            exp.Window: 2,         # Window functions are complex\n",
    "            exp.Connector: 1,      # AND / OR\n",
    "            exp.Subquery: 1        # Base penalty for existence of subquery\n",
    "        }\n",
    "        \n",
    "        # Configuration for Semantic Penalty\n",
    "        self.semantic_weight = 0.5  # Alpha in the formula\n",
    "        self.min_alias_length = 3\n",
    "        self.generic_aliases = {'temp', 'data', 't', 'x', 'val', 'obj', 'row'}\n",
    "        self.analyzer = SQLAnalyzer()\n",
    "\n",
    "    def calculate(self, sql: str):\n",
    "        \"\"\"\n",
    "        Parses SQL and returns the SSCS score along with a detailed breakdown.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            parsed = sqlglot.parse_one(sql)\n",
    "        except Exception as e:\n",
    "            return {\"error\": f\"Parse Error: {e}\"}\n",
    "\n",
    "        # 1. Isolate CTEs and Main Query\n",
    "        ctes = []\n",
    "        main_query = parsed\n",
    "\n",
    "        # If there is a WITH clause, extract CTEs\n",
    "        if parsed.find(exp.CTE):\n",
    "            # We treat CTEs as independent \"functions\" for complexity\n",
    "            # Note: sqlglot stores CTEs in the 'with' arg of the main expression\n",
    "            ctes = parsed.find_all(exp.CTE)\n",
    "                # We analyze the main query as if the CTEs are just tables\n",
    "                # (The complexity of defining the CTE is handled separately)\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        # 2. Calculate Structural Complexity (C_struct)\n",
    "        # Sum of CTE complexities + Main Query complexity\n",
    "        struct_score = 0\n",
    "        component_scores = []\n",
    "        sscs_scores = {}\n",
    "\n",
    "        # Analyze CTEs (Depth starts at 0 for each, promoting modularity)\n",
    "        for cte in ctes:\n",
    "            cte_score = self._compute_structural_score(cte.this, depth=0)\n",
    "            struct_score += cte_score\n",
    "            component_scores.append(f\"CTE '{cte.alias}': {cte_score}\")\n",
    "            cte_penalty, _ = self._compute_semantic_penalty(cte.this)\n",
    "            cte_sscs = cte_score * (1 + cte_penalty)\n",
    "            sscs_scores[cte.alias] = {\n",
    "                \"SSCS\": round(cte_sscs, 2),\n",
    "                \"Structural Score\": round(cte_score, 2),\n",
    "                \"Semantic Penalty\": round(cte_penalty, 2)\n",
    "            }\n",
    "\n",
    "        # Analyze Main Query (Depth starts at 0)\n",
    "        # We explicitly exclude the WITH clause from traversal to avoid double counting\n",
    "        main_score = self._compute_structural_score(main_query, depth=0, exclude_node=exp.With)\n",
    "        struct_score += main_score\n",
    "        component_scores.append(f\"Main Query: {main_score}\")\n",
    "        main_penalty, _ = self._compute_semantic_penalty(main_query)\n",
    "        main_sscs = main_score * (1 + main_penalty)\n",
    "\n",
    "        # 3. Calculate Semantic Penalty (P_sem)\n",
    "        # We look at all aliases across the entire parsed tree globally\n",
    "        semantic_penalty, alias_stats = self._compute_semantic_penalty(parsed)\n",
    "\n",
    "        main_sscs = main_score * (1 + semantic_penalty)\n",
    "\n",
    "        # 4. Final Formula: SSCS = C_struct * (1 + P_sem)\n",
    "        final_sscs = struct_score * (1 + semantic_penalty)\n",
    "\n",
    "        sscs_scores[\"final SELECT\"] = {\n",
    "            \"SSCS\": round(main_sscs, 2),\n",
    "            \"Structural Score\": round(main_score, 2),\n",
    "            \"Semantic Penalty\": round(semantic_penalty, 2)\n",
    "        }\n",
    "\n",
    "        sscs_scores[\"overall\"] = {\n",
    "            \"SSCS\": round(final_sscs, 2),\n",
    "            \"Structural Score\": round(struct_score, 2),\n",
    "            \"Semantic Penalty\": round(semantic_penalty, 2)\n",
    "        }\n",
    "\n",
    "        return {\n",
    "            \"sscs_scores\": sscs_scores,\n",
    "            \"breakdown\": component_scores,\n",
    "            \"alias_analysis\": alias_stats\n",
    "        }\n",
    "    \n",
    "    def _compute_structural_score(self, node, depth, exclude_node=None):\n",
    "        \"\"\"\n",
    "        Recursive visitor to calculate complexity weights based on AST nodes.\n",
    "        Increases depth penalty for nested subqueries.\n",
    "        \"\"\"\n",
    "        score = 0\n",
    "        \n",
    "        # If this node is the one we want to exclude (e.g. the CTE definitions block), stop recursion\n",
    "        if exclude_node and isinstance(node, exclude_node):\n",
    "            return 0\n",
    "\n",
    "        # Apply Weight if node type is in our config\n",
    "        if type(node) in self.weights:\n",
    "            base_weight = self.weights[type(node)]\n",
    "            # Formula: Weight + Depth Penalty\n",
    "            # We add depth to the weight. Deeper logic is heavier.\n",
    "            score += base_weight + (0.5 * depth)\n",
    "\n",
    "        # Check for nesting triggers\n",
    "        # If we enter a Subquery (SELECT inside FROM/WHERE), increment depth\n",
    "        next_depth = depth\n",
    "        if isinstance(node, exp.Subquery):\n",
    "            next_depth += 1\n",
    "        \n",
    "        # Recursively visit children\n",
    "        # sqlglot's args.values() gives us lists of children or single children\n",
    "        for child_list in node.args.values():\n",
    "            if isinstance(child_list, list):\n",
    "                for child in child_list:\n",
    "                    if isinstance(child, exp.Expression):\n",
    "                        score += self._compute_structural_score(child, next_depth, exclude_node)\n",
    "            elif isinstance(child_list, exp.Expression):\n",
    "                score += self._compute_structural_score(child_list, next_depth, exclude_node)\n",
    "                \n",
    "        return score\n",
    "    \n",
    "    def _compute_semantic_penalty(self, root_node, threshold=0.7):\n",
    "        \"\"\"\n",
    "        Uses semantic similarity analysis to compute penalty for poorly named aliases.\n",
    "        \n",
    "        Converts the parsed AST back to SQL, analyzes it with the SQLAnalyzer,\n",
    "        and penalizes aliases with low similarity scores between the alias name\n",
    "        and the code they represent.\n",
    "        \"\"\"\n",
    "        # Convert parsed node back to SQL string for analyzer\n",
    "        sql_str = root_node.sql()\n",
    "        \n",
    "        # Get semantic analysis from the analyzer\n",
    "        analysis = self.analyzer.analyze(sql_str, threshold=threshold)\n",
    "        \n",
    "        if not analysis:\n",
    "            return 0.0, {\"total\": 0, \"low_similarity\": [], \"average_similarity\": 0.0}\n",
    "        \n",
    "        # Calculate penalty based on similarity scores\n",
    "        # Lower similarity = higher penalty\n",
    "        similarities = [item['similarity'] for item in analysis]\n",
    "        low_similarity_items = [item for item in analysis if item['similarity'] < threshold]\n",
    "        \n",
    "        # Penalty formula: average the \"badness\" (1 - similarity) for items below threshold\n",
    "        if low_similarity_items:\n",
    "            avg_badness = sum(1.0 - item['similarity'] for item in low_similarity_items) / len(analysis)\n",
    "            penalty = self.semantic_weight * avg_badness\n",
    "        else:\n",
    "            penalty = 0.0\n",
    "        \n",
    "        avg_similarity = sum(similarities) / len(similarities) if similarities else 0.0\n",
    "        \n",
    "        return penalty, {\n",
    "            \"total\": len(analysis), \n",
    "            \"low_similarity_count\": len(low_similarity_items),\n",
    "            \"low_similarity_examples\": [\n",
    "                {\n",
    "                    \"alias\": item['alias'], \n",
    "                    \"similarity\": round(item['similarity'], 3),\n",
    "                    \"type\": item['type']\n",
    "                } \n",
    "                for item in low_similarity_items[:5]\n",
    "            ],\n",
    "            \"average_similarity\": round(avg_similarity, 3)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e3a7790",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Alias 'total_rev' appears to be a poor name for the code (similarity=0.460)\n",
      "\n",
      "code:\n",
      "SUM(o.amount)\n",
      "\n",
      "Alias 'c' appears to be a poor name for the code (similarity=0.414)\n",
      "\n",
      "code:\n",
      "customers AS c\n",
      "\n",
      "Alias 'o' appears to be a poor name for the code (similarity=0.484)\n",
      "\n",
      "code:\n",
      "orders AS o\n",
      "\n",
      "Alias 'r' appears to be a poor name for the code (similarity=0.356)\n",
      "\n",
      "code:\n",
      "revenue_cte AS r\n",
      "\n",
      "Alias 'revenue_cte' appears to be a poor name for the code (similarity=0.414)\n",
      "\n",
      "code:\n",
      "SELECT c.id, SUM(o.amount) AS total_rev FROM customers AS c JOIN orders AS o ON c.id = o.customer_id GROUP BY 1\n",
      "\n",
      "Alias 'risky_users' appears to be a poor name for the code (similarity=0.270)\n",
      "\n",
      "code:\n",
      "SELECT id FROM revenue_cte AS r WHERE r.total_rev > 10000 AND (CASE WHEN r.total_rev > 50000 THEN 1 ELSE 0 END) = 1\n",
      "\n",
      "Alias 'c' appears to be a poor name for the code (similarity=0.304)\n",
      "\n",
      "code:\n",
      "COUNT(*)\n",
      "\n",
      "Alias 'total_rev' appears to be a poor name for the code (similarity=0.460)\n",
      "\n",
      "code:\n",
      "SUM(o.amount)\n",
      "\n",
      "Alias 't1' appears to be a poor name for the code (similarity=0.535)\n",
      "\n",
      "code:\n",
      "revenue_cte AS t1\n",
      "\n",
      "Alias 'c' appears to be a poor name for the code (similarity=0.414)\n",
      "\n",
      "code:\n",
      "customers AS c\n",
      "\n",
      "Alias 'o' appears to be a poor name for the code (similarity=0.484)\n",
      "\n",
      "code:\n",
      "orders AS o\n",
      "\n",
      "Alias 'r' appears to be a poor name for the code (similarity=0.356)\n",
      "\n",
      "code:\n",
      "revenue_cte AS r\n",
      "\n",
      "Alias 'revenue_cte' appears to be a poor name for the code (similarity=0.414)\n",
      "\n",
      "code:\n",
      "SELECT c.id, SUM(o.amount) AS total_rev FROM customers AS c JOIN orders AS o ON c.id = o.customer_id GROUP BY 1\n",
      "\n",
      "Alias 'risky_users' appears to be a poor name for the code (similarity=0.270)\n",
      "\n",
      "code:\n",
      "SELECT id FROM revenue_cte AS r WHERE r.total_rev > 10000 AND (CASE WHEN r.total_rev > 50000 THEN 1 ELSE 0 END) = 1\n",
      "\n",
      "Alias 'c' appears to be a poor name for the code (similarity=0.304)\n",
      "\n",
      "code:\n",
      "COUNT(*)\n",
      "\n",
      "Alias 'total_rev' appears to be a poor name for the code (similarity=0.460)\n",
      "\n",
      "code:\n",
      "SUM(o.amount)\n",
      "\n",
      "Alias 't1' appears to be a poor name for the code (similarity=0.535)\n",
      "\n",
      "code:\n",
      "revenue_cte AS t1\n",
      "\n",
      "Alias 'c' appears to be a poor name for the code (similarity=0.414)\n",
      "\n",
      "code:\n",
      "customers AS c\n",
      "\n",
      "Alias 'o' appears to be a poor name for the code (similarity=0.484)\n",
      "\n",
      "code:\n",
      "orders AS o\n",
      "\n",
      "Alias 'r' appears to be a poor name for the code (similarity=0.356)\n",
      "\n",
      "code:\n",
      "revenue_cte AS r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Example Usage ---\n",
    "\n",
    "complex_sql = \"\"\"\n",
    "WITH revenue_cte AS (\n",
    "    SELECT \n",
    "        c.id, \n",
    "        sum(o.amount) as total_rev\n",
    "    FROM customers c\n",
    "    JOIN orders o ON c.id = o.customer_id\n",
    "    GROUP BY 1\n",
    "),\n",
    "risky_users AS (\n",
    "    SELECT \n",
    "        id \n",
    "    FROM revenue_cte r\n",
    "    WHERE r.total_rev > 10000 \n",
    "      AND (CASE WHEN r.total_rev > 50000 THEN 1 ELSE 0 END) = 1\n",
    ")\n",
    "SELECT \n",
    "    t1.id, \n",
    "    t1.total_rev\n",
    "FROM revenue_cte t1\n",
    "LEFT JOIN (\n",
    "    SELECT user_id, count(*) as c FROM logs GROUP BY 1\n",
    ") t2 ON t1.id = t2.user_id\n",
    "WHERE t1.id IN (SELECT id FROM risky_users)\n",
    "\"\"\"\n",
    "\n",
    "calc = SSCSCalculator()\n",
    "result = calc.calculate(complex_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a61953e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'revenue_cte': {'SSCS': 2.55,\n",
       "  'Structural Score': 2.0,\n",
       "  'Semantic Penalty': 0.27},\n",
       " 'risky_users': {'SSCS': 3.97,\n",
       "  'Structural Score': 3.0,\n",
       "  'Semantic Penalty': 0.32},\n",
       " 'final SELECT': {'SSCS': 5.84,\n",
       "  'Structural Score': 4.5,\n",
       "  'Semantic Penalty': 0.3},\n",
       " 'overall': {'SSCS': 12.33, 'Structural Score': 9.5, 'Semantic Penalty': 0.3}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['sscs_scores']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
